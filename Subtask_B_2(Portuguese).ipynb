{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Subtask B (Portuguese).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Bertram with portuguese-based-bert"
      ],
      "metadata": {
        "id": "jp0wWzpAYXEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### helper and processing"
      ],
      "metadata": {
        "id": "QXZQ_XokZfD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/H-TayyarMadabushi/AStitchInLanguageModels.git"
      ],
      "metadata": {
        "id": "Sv9zU-RiYYoc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c5ff3e7-b572-4623-a864-c5a2476eec4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AStitchInLanguageModels'...\n",
            "remote: Enumerating objects: 1030, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 1030 (delta 11), reused 4 (delta 4), pack-reused 1013\u001b[K\n",
            "Receiving objects: 100% (1030/1030), 79.59 MiB | 32.19 MiB/s, done.\n",
            "Resolving deltas: 100% (394/394), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd AStitchInLanguageModels/dependencies/sentence-transformers\n",
        "# !pip install -e . \n",
        "# %cd /content/"
      ],
      "metadata": {
        "id": "OtYrxJIRcdLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd transformers/\n",
        "!pip install --editable .\n",
        "%cd /content/ "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoaGqLZqcfZI",
        "outputId": "3c1f104a-e2a9-46b6-b78b-e36c6cd201e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 93680, done.\u001b[K\n",
            "remote: Counting objects: 100% (233/233), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 93680 (delta 164), reused 191 (delta 134), pack-reused 93447\u001b[K\n",
            "Receiving objects: 100% (93680/93680), 85.89 MiB | 14.90 MiB/s, done.\n",
            "Resolving deltas: 100% (68686/68686), done.\n",
            "/content/transformers\n",
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 10.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 68.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0.dev0) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.0.dev0) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 tokenizers-0.12.1 transformers\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRzexhR1cgZ1",
        "outputId": "6af3dcdb-9b63-4ee9-fb67-9388f8acc3a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 50.9 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 18.9 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.1.0 frozenlist-1.3.0 fsspec-2022.3.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=02a3c0f6d954a295ccc4ae18b8d23e437c4c1af2a7d9033654f66766b538dcae\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 transformers-4.18.0\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 34.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (4.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.0.53)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.11.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=462e1cbaa96a7bf40f55f94d7df511de7c9918ee7f6c997199ee8c02e62df88b\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.0 sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/H-TayyarMadabushi/SemEval_2022_Task2-idiomaticity.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6n5TvRNcls7",
        "outputId": "14e6fb26-14ad-45cd-be9f-6af159a179c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SemEval_2022_Task2-idiomaticity'...\n",
            "remote: Enumerating objects: 123, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 123 (delta 48), reused 61 (delta 15), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (123/123), 2.50 MiB | 10.13 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/timoschick/bertram.git\n",
        "!git clone https://github.com/timoschick/form-context-model.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSe5t0ILc2tX",
        "outputId": "156a8e46-c2a0-4ddd-95d0-23d5130d2ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'bertram' already exists and is not an empty directory.\n",
            "fatal: destination path 'form-context-model' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import sys\n",
        "import csv\n",
        "import gzip\n",
        "import math\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from datetime                         import datetime\n",
        "from torch.utils.data                 import DataLoader\n",
        "from sklearn.metrics.pairwise         import paired_cosine_distances\n",
        "\n",
        "from datasets                         import load_dataset\n",
        "from transformers                     import AutoModelForMaskedLM\n",
        "from transformers                     import AutoTokenizer\n",
        "from sentence_transformers            import SentenceTransformer,  LoggingHandler, losses, models, util\n",
        "from sentence_transformers.readers    import InputExample\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "sys.path.append( '/content/SemEval_2022_Task2-idiomaticity/SubTaskB/' )\n",
        "sys.path.append('bertram/')\n",
        "sys.path.append('form-context-model/')\n",
        "from SubTask2Evaluator                import evaluate_submission"
      ],
      "metadata": {
        "id": "h3OTaGfjct0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "languages=['PT']\n",
        "def _parse_train_data( train_data_location, languages, tokenize=True ) :\n",
        "\n",
        "  header, train_data = load_csv( train_data_location )\n",
        "  \n",
        "  train_data_with_labels                 = list()\n",
        "  train_data_requiring_labels            = list()\n",
        "  need_predictions_for_train_data_labels = list()\n",
        "\n",
        "  # ['ID', 'MWE1', 'MWE2', 'Language', 'sentence_1', 'sentence_2', 'sim', 'alternative_1', 'alternative_2']\n",
        "\n",
        "  skipped = 0 \n",
        "  for elem in train_data :\n",
        "\n",
        "    if not elem[ header.index( 'Language' ) ] in languages :\n",
        "      skipped += 1\n",
        "      continue\n",
        "\n",
        "    mwe1          = elem[ header.index( 'MWE1'          ) ] \n",
        "    mwe2          = elem[ header.index( 'MWE2'          ) ] \n",
        "    \n",
        "    this_sim      = elem[ header.index( 'sim'           ) ]\n",
        "    sentence_1    = elem[ header.index( 'sentence_1'    ) ]\n",
        "    sentence_2    = elem[ header.index( 'sentence_2'    ) ]\n",
        "    alternative_1 = elem[ header.index( 'alternative_1' ) ]\n",
        "    alternative_2 = elem[ header.index( 'alternative_2' ) ]\n",
        "\n",
        "    ## Remove below if you do not want to tokenize with idiom tokens!\n",
        "    if tokenize : \n",
        "      if mwe1 != 'None' : \n",
        "        replaced = re.sub( mwe1, tokenise_idiom( mwe1 ), sentence_1, flags=re.I)\n",
        "        assert replaced != sentence_1\n",
        "        sentence_1 = replaced\n",
        "      if mwe2 != 'None' : \n",
        "        replaced = re.sub( mwe1, tokenise_idiom( mwe2 ), sentence_2, flags=re.I)\n",
        "        assert replaced != sentence_2\n",
        "        sentence_2 = replaced\n",
        "  \n",
        "   \n",
        "    if this_sim != 'None' :\n",
        "      tmp = float( this_sim ) \n",
        "      train_data_with_labels.append( [ sentence_1, sentence_2, this_sim ] ) \n",
        "      continue\n",
        "    train_data_requiring_labels.append( [ sentence_1, sentence_2 ] ) \n",
        "    need_predictions_for_train_data_labels.append( [ alternative_1, alternative_2 ] )\n",
        "\n",
        "  assert len( need_predictions_for_train_data_labels ) == len( train_data_requiring_labels )\n",
        "  assert len( train_data ) == len( need_predictions_for_train_data_labels ) + len( train_data_with_labels ) + skipped\n",
        "\n",
        "  return train_data_with_labels, train_data_requiring_labels, need_predictions_for_train_data_labels "
      ],
      "metadata": {
        "id": "3DAe4sqrdLTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv( path ) : \n",
        "  header = None\n",
        "  data   = list()\n",
        "  with open( path, encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader( csvfile ) \n",
        "    for row in reader : \n",
        "      if header is None : \n",
        "        header = row\n",
        "        continue\n",
        "      data.append( row ) \n",
        "  return header, data\n",
        "def tokenise_idiom( phrase ) :\n",
        "  return 'ID' + re.sub( r'[\\s|-]', '', phrase ).lower() + 'ID'"
      ],
      "metadata": {
        "id": "QFbW7z5ueAQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_eval_data( location, languages, test_print=False ) :\n",
        "  header, data = load_csv( location )\n",
        "  sentence1s = list()\n",
        "  sentence2s = list()\n",
        "  for elem in data : \n",
        "    if not languages is None and not elem[ header.index( 'Language' ) ] in languages : \n",
        "      continue\n",
        "    sentence1 = elem[ header.index( 'sentence1' ) ] \n",
        "    sentence2 = elem[ header.index( 'sentence2' ) ] \n",
        "    mwe1      = elem[ header.index( 'MWE1'      ) ] \n",
        "    mwe2      = elem[ header.index( 'MWE2'      ) ] \n",
        "\n",
        "    if test_print : \n",
        "      print( sentence1 ) \n",
        "      print( sentence2 ) \n",
        "      print( mwe1 ) \n",
        "      print( mwe2 ) \n",
        "\n",
        "    if mwe1 != 'None' : \n",
        "      replaced = re.sub( mwe1, tokenise_idiom( mwe1 ), sentence1, flags=re.I)\n",
        "      assert replaced != sentence1\n",
        "      sentence1 = replaced\n",
        "    if mwe2 != 'None' : \n",
        "      replaced = re.sub( mwe1, tokenise_idiom( mwe2 ), sentence2, flags=re.I)\n",
        "      assert replaced != sentence2\n",
        "      sentence2 = replaced\n",
        "\n",
        "    if test_print : \n",
        "      print( sentence1 ) \n",
        "      print( sentence2 ) \n",
        "      break\n",
        "\n",
        "    sentence1s.append( sentence1 ) \n",
        "    sentence2s.append( sentence2 ) \n",
        "\n",
        "  return sentence1s, sentence2s\n",
        "\n",
        "\n",
        "def get_similarities( location, model, languages=None ) : \n",
        "  sentences1, sentences2 = prepare_eval_data( location, languages ) \n",
        "\n",
        "  #Compute embedding for both lists\n",
        "  embeddings1 = model.encode(sentences1, show_progress_bar=True, convert_to_numpy=True)\n",
        "  embeddings2 = model.encode(sentences2, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "  # Compute cosine-similarits\n",
        "  cosine_scores = 1 - (paired_cosine_distances(embeddings1, embeddings2))\n",
        "\n",
        "  return cosine_scores"
      ],
      "metadata": {
        "id": "jFUfjRDI2Zjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_to_submission( languages, settings, sims, location ) : \n",
        "  header, data = load_csv( location ) \n",
        "  sims = list( reversed( sims ) )\n",
        "  ## Validate with length\n",
        "  updatable = [ i for i in data if i[ header.index( 'Language' ) ] in languages and i[ header.index( 'Setting' ) ] in settings ]\n",
        "  assert len( updatable ) == len( sims ) \n",
        "\n",
        "  ## Will update in sequence - if data is not in sequence must update one language / setting at a time. \n",
        "  started_update = False\n",
        "  for elem in data : \n",
        "    if elem[ header.index( 'Language' ) ] in languages and elem[ header.index( 'Setting' ) ] in settings : \n",
        "      sim_to_insert = sims.pop()\n",
        "      elem[-1] = sim_to_insert\n",
        "      started_update = True\n",
        "    else :  \n",
        "      assert not started_update ## Once we start, we must complete. \n",
        "    if len( sims ) == 0 : \n",
        "      break \n",
        "  assert len( sims ) == 0 ## Should be done here. \n",
        "\n",
        "  return [ header ] + data\n",
        "\n",
        "def write_csv( data, location ) : \n",
        "  with open( location, 'w', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer( csvfile ) \n",
        "    writer.writerows( data ) \n",
        "  print( \"Wrote {}\".format( location ) ) \n",
        "  return"
      ],
      "metadata": {
        "id": "kzwWmGEa2eqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_predictions_for_train_data_labels( model_path, data ) :\n",
        "\n",
        "  model      = SentenceTransformer( model_path )\n",
        "\n",
        "  sentences1 = [ i[0] for i in data ]\n",
        "  sentences2 = [ i[1] for i in data ]\n",
        "\n",
        "  embeddings1 = model.encode(sentences1, show_progress_bar=True, convert_to_numpy=True)\n",
        "  embeddings2 = model.encode(sentences2, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "  cosine_scores = 1 - (paired_cosine_distances(embeddings1, embeddings2))\n",
        "\n",
        "  return cosine_scores\n",
        "\n",
        "def generate_train_data( train_data_location, model_path, languages ) :\n",
        "  \n",
        "  train_data_with_labels, train_data_requiring_labels, need_predictions_for_train_data_labels = _parse_train_data( train_data_location, languages )\n",
        "  sims = _get_predictions_for_train_data_labels( model_path, need_predictions_for_train_data_labels )\n",
        "\n",
        "  train_data_requiring_labels_with_labels = list()\n",
        "  for index in range( len( train_data_requiring_labels ) ) : \n",
        "    train_data_requiring_labels_with_labels.append( [ train_data_requiring_labels[index][0], train_data_requiring_labels[index][1], sims[index] ] )\n",
        "\n",
        "  train_data = [ [ 'sentence_1', 'sentence_2', 'sim' ] ] + train_data_with_labels + train_data_requiring_labels_with_labels\n",
        "  assert all( [ (len(i) == 3) for i in train_data ] )\n",
        "  \n",
        "  return train_data"
      ],
      "metadata": {
        "id": "C_svoO2U4K8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_location = '/content/SemEval_2022_Task2-idiomaticity/SubTaskB/EvaluationData/'\n",
        "outpath = '/content/models/'\n",
        "dev_location                = os.path.join( data_location, 'dev.csv'                     ) \n",
        "eval_location               = os.path.join( data_location, 'eval.csv'                    ) \n",
        "dev_formated_file_location  = os.path.join( data_location, 'dev.submission_format.csv'   ) \n",
        "## Save tmp model here.  \n",
        "outdir = os.path.join( outpath, 'pBERT' + '-' + str( 4 ) ) "
      ],
      "metadata": {
        "id": "pASs0RpJ2toS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models"
      ],
      "metadata": {
        "id": "snSdHrLe3tcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5r3jnzpnvfR",
        "outputId": "3aaab0e6-f6b2-4a36-bcfc-00fa7ffee404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_location = 'SemEval_2022_Task2-idiomaticity/SubTaskB/TrainData/train_data.csv'"
      ],
      "metadata": {
        "id": "iMyTeCFf22P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_with_labels, train_data_requiring_labels, need_predictions_for_train_data_labels = _parse_train_data( train_data_location, languages )"
      ],
      "metadata": {
        "id": "QbuOofn-2yyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idioms = list()\n",
        "for data_split in [ 'dev', 'eval' ] : \n",
        "  file_path = os.path.join( data_location, data_split + '.csv' )\n",
        "  header, data = load_csv( file_path )\n",
        "  for elem in data : \n",
        "    if elem[ header.index( 'Language' ) ] =='PT' :\n",
        "      idioms.append( elem[ header.index( 'MWE1' ) ] )\n",
        "      idioms.append( elem[ header.index( 'MWE2' ) ] )\n",
        "\n",
        "idioms = list( set( idioms ) ) \n",
        "idioms.remove( 'None' )"
      ],
      "metadata": {
        "id": "-qy3DISJ29X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idioms = [ tokenise_idiom( i ) for i in idioms ]"
      ],
      "metadata": {
        "id": "44aQzpn92-DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **More Training data**"
      ],
      "metadata": {
        "id": "HOzmQKr6FdB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pcocessing training data for bertram"
      ],
      "metadata": {
        "id": "6oCU4vdXZmDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.statmt.org/cc-100/pt.txt.xz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYTQNWHqEInZ",
        "outputId": "5a560fdd-e2f3-4672-9838-89e95e6b6467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-07 02:43:04--  https://data.statmt.org/cc-100/pt.txt.xz\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13677690424 (13G) [application/x-xz]\n",
            "Saving to: ‘pt.txt.xz’\n",
            "\n",
            "pt.txt.xz             0%[                    ]  21.49M  6.57MB/s    eta 33m 54s"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.statmt.org/cc-100/gl.txt.xz"
      ],
      "metadata": {
        "id": "lVTz_sd1FLLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install xz-utils"
      ],
      "metadata": {
        "id": "SZai8vunKlUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!xz -d gl.txt.xz"
      ],
      "metadata": {
        "id": "U9x6xI6kL5QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpula=[]\n",
        "num=0\n",
        "with open('gl.txt','r') as f:\n",
        "  for line in f.readlines():\n",
        "    if num==1000000:\n",
        "      break\n",
        "    corpula.append(line)\n",
        "    num+=1\n",
        "f.close()\n",
        "f=open(\"train_corpus_l.txt\",\"w\")\n",
        "f.writelines(corpula)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "rERQhNq-NEj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir train_data"
      ],
      "metadata": {
        "id": "IimlZWtfExHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python form-context-model/fcm/preprocess.py train --input train_corpus_l.txt --output train_data/"
      ],
      "metadata": {
        "id": "rLrU8gB6MOF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir train_data/buckets\n",
        "!mv train_data/train.bucket* train_data/buckets/"
      ],
      "metadata": {
        "id": "YlNcwfJFMxqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# some code change in the train.py() get_linear_schedule_with_warmup num_warmup_steps num_training_steps and exchange the sequence for betram.py line 193\n",
        "!python3 bertram/train.py \\\n",
        "   --model_cls bert \\\n",
        "   --bert_model bert-base-uncased \\\n",
        "   --output_dir outputs/gl_1m \\\n",
        "   --train_dir train_data/buckets/ \\\n",
        "   --vocab train_data/train.vwc100 \\\n",
        "   --emb_file embeddings-bert-base-uncased.txt \\\n",
        "   --num_train_epochs 2 \\\n",
        "   --emb_dim 768 \\\n",
        "   --train_batch_size 16 \\\n",
        "   --smin 1 \\\n",
        "   --smax 1 \\\n",
        "   --max_seq_length 32 \\\n",
        "   --mode form \\\n",
        "   --learning_rate 0.01 \\\n",
        "   --dropout 0.1 \\"
      ],
      "metadata": {
        "id": "-vjTykVjQA7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### move betram to drive"
      ],
      "metadata": {
        "id": "MooYpbQJ04Fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "rueEb_9hBjOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir outputs\n",
        "# !cp -r /content/gdrive/MyDrive/idiom_detection/models/bertram/* /content/outputs"
      ],
      "metadata": {
        "id": "O8lWfACeBr-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !mkdir -p /content/gdrive/MyDrive/idiom_detection/models/bertram\n",
        "# !cp -r /content/outputs/* /content/gdrive/MyDrive/idiom_detection/models/bertram"
      ],
      "metadata": {
        "id": "RNiS3yeIhfvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Incorporate bertram embedding"
      ],
      "metadata": {
        "id": "8JaalS_PZqjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertram import BertramWrapper"
      ],
      "metadata": {
        "id": "ouUc_23vQ2tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertram = BertramWrapper('outputs/gl-e1', device='cpu')"
      ],
      "metadata": {
        "id": "WuTnguoDh6nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='neuralmind/bert-base-portuguese-cased'\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, truncation=True)"
      ],
      "metadata": {
        "id": "pWIfgHfMh6ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_with_contexts ={}\n",
        "for i in idioms:\n",
        "  words_with_contexts[i]=[]\n",
        "embeddings={word: bertram.infer_vector(word, contexts) for word, contexts in words_with_contexts.items()}"
      ],
      "metadata": {
        "id": "2UQ5-_zsh6yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens=[f\"<BERTRAM:{word}>\" for word in embeddings.keys()]\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})"
      ],
      "metadata": {
        "id": "9JOdewvgiuSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_weight = model.bert.embeddings.word_embeddings.weight\n",
        "max_id =max(tokenizer.additional_special_tokens_ids)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "3RCaBv3vizto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.bert.embeddings.word_embeddings.weight.requires_grad=False"
      ],
      "metadata": {
        "id": "Dky_SKJAi6Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, embedding in embeddings.items():\n",
        "      word_id = tokenizer.convert_tokens_to_ids(f\"<BERTRAM:{word}>\")\n",
        "      model.bert.embeddings.word_embeddings.weight[word_id] = embedding"
      ],
      "metadata": {
        "id": "zzipTVe9i7Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained( outdir )\n",
        "tokenizer.save_pretrained(outdir)"
      ],
      "metadata": {
        "id": "dNjVESLhi74i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre_train model"
      ],
      "metadata": {
        "id": "49rn8fEMZvae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples = []\n",
        "dev_samples   = []\n",
        "test_samples  = []\n",
        "for split in [ 'train', 'validation', 'test' ] :\n",
        "      dataset = load_dataset( 'assin2', split=split, ignore_verifications=True )\n",
        "      for elem in dataset :\n",
        "        ## {'entailment_judgment': 1, 'hypothesis': 'Uma criança está segurando uma pistola de água', 'premise': 'Uma criança risonha está segurando uma pistola de água e sendo espirrada com água', 'relatedness_score': 4.5, 'sentence_pair_id': 1}\n",
        "          score = float( elem['relatedness_score'] ) / 5.0 # Normalize score to range 0 ... 1\n",
        "          inp_example = InputExample(texts=[elem['hypothesis'], elem['premise']], label=score)\n",
        "          if split == 'validation':\n",
        "            dev_samples.append(inp_example)\n",
        "          elif split == 'test':\n",
        "            test_samples.append(inp_example)\n",
        "          elif split == 'train' :\n",
        "            train_samples.append(inp_example)\n",
        "          else :\n",
        "              raise Exception( \"Unknown split. Should be one of ['train', 'test', 'validation'].\" )"
      ],
      "metadata": {
        "id": "tLKtKriA2KbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = outdir\n",
        "sent_trans_path='models/psentBert-4/'\n",
        "word_embedding_model = models.Transformer(model_path)\n",
        "\n",
        "# Apply mean pooling to get one fixed sized sentence vector\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "                                pooling_mode_mean_tokens=True,\n",
        "                                pooling_mode_cls_token=False,\n",
        "                                pooling_mode_max_tokens=False)\n",
        "\n",
        "\n",
        "tokenizer      = AutoTokenizer.from_pretrained(\n",
        "  model_path             , \n",
        "  use_fast       = False ,\n",
        "  max_length     = 510   ,\n",
        "  force_download = True\n",
        ")\n",
        "\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "model._first_module().tokenizer = tokenizer\n",
        "\n",
        "model.save( sent_trans_path )\n"
      ],
      "metadata": {
        "id": "Vyp0xcZHjE26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 8\n",
        "num_epochs       = 2\n",
        "model_save_path='models/PnosentBert-8'\n",
        "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
        "train_loss       = losses.CosineSimilarityLoss(model=model)\n",
        "  \n",
        "evaluator        = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
        "\n",
        "# Configure the training. \n",
        "warmup_steps     = math.ceil(len(train_dataloader) * num_epochs  * 0.1) #10% of train data for warm-up\n",
        "print(\"Warmup-steps: {}\".format(warmup_steps), flush=True)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          evaluator=evaluator,\n",
        "          epochs=num_epochs,\n",
        "          evaluation_steps=1000,\n",
        "          warmup_steps=warmup_steps,\n",
        "          output_path=model_save_path\n",
        ")\n",
        "\n",
        "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='sts-test')\n",
        "test_evaluator(model, output_path=model_save_path)\n",
        "\n",
        "model_path = model_save_path"
      ],
      "metadata": {
        "id": "suDAByJxjI4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('models/PnosentBert-8')\n",
        "dev_sims  = get_similarities( dev_location , model, languages ) "
      ],
      "metadata": {
        "id": "wYM8WnCJjT1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create submission file on the development set. \n",
        "submission_data = insert_to_submission( languages, [ 'pre_train' ], dev_sims, dev_formated_file_location )  \n",
        "results_file    = os.path.join( outpath, 'dev.pre_train_results-' + str( 4 ) + '.csv' )\n",
        "write_csv( submission_data, results_file )\n",
        "\n",
        "## Evaluate development set. \n",
        "results = evaluate_submission( results_file, os.path.join( data_location, 'dev.gold.csv' ) )"
      ],
      "metadata": {
        "id": "JPWxyJykjWFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "zh3rvSeejte0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine_tune"
      ],
      "metadata": {
        "id": "2acnn24jZ6fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir gdrive/MyDrive/idiom_detection/models/large\n",
        "# !cp models/PnosentBert-8/* gdrive/MyDrive/idiom_detection/models/large/"
      ],
      "metadata": {
        "id": "TJZ4bozC4VAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_pre_train_seed = 4 ## Found this by running above (as in pre-train setting) multiple times. \n",
        "\n",
        "train_data_location = 'SemEval_2022_Task2-idiomaticity/SubTaskB/TrainData/train_data.csv'\n",
        "out_location        = 'models/FineTune/'\n",
        "\n",
        "model_path = '/content/models/PnosentBert-8/'\n",
        "train_data = generate_train_data( train_data_location, model_path, languages )"
      ],
      "metadata": {
        "id": "w22HeqYVYf-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_torch_available() :\n",
        "    try:\n",
        "        import torch\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "def is_tf_available() :\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "def set_seed(seed: int):\n",
        "  \"\"\"\n",
        "  Modified from : https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_utils.py\n",
        "  Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
        "  installed).\n",
        "  Args:\n",
        "      seed (:obj:`int`): The seed to set.\n",
        "  \"\"\"\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if is_torch_available():\n",
        "      torch.manual_seed(seed)\n",
        "      torch.cuda.manual_seed_all(seed)\n",
        "      # ^^ safe to call this function even if cuda is not available\n",
        "\n",
        "      ## From https://pytorch.org/docs/stable/notes/randomness.html\n",
        "      torch.backends.cudnn.benchmark = False\n",
        "\n",
        "      ## Might want to use the following, but set CUBLAS_WORKSPACE_CONFIG=:16:8\n",
        "      # try : \n",
        "      #   torch.use_deterministic_algorithms(True)\n",
        "      # except AttributeError: \n",
        "      #   torch.set_deterministic( True )\n",
        "      \n",
        "  if is_tf_available():\n",
        "      import tensorflow as tf\n",
        "      tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "_vEvtMf5YpAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_eval_subtask_b_fine_tune( \n",
        "    model_path, \n",
        "    seed, \n",
        "    data_location, \n",
        "    dev_formated_file_location,\n",
        "    eval_formated_file_location,\n",
        "    train_data, \n",
        "    out_location, \n",
        "    languages, \n",
        "    epoch=None \n",
        "    ) :\n",
        "\n",
        "  set_seed( seed )\n",
        "  \n",
        "  dev_location                = os.path.join( data_location, 'dev.csv'                     ) \n",
        "  eval_location               = os.path.join( data_location, 'eval.csv'                    ) \n",
        "\n",
        "\n",
        "  ## Training Dataloader\n",
        "  train_samples = list()\n",
        "\n",
        "  header     = train_data[0] ## ['sentence_1', 'sentence_2', 'sim']\n",
        "  train_data = train_data[1:]\n",
        "  for elem in train_data :\n",
        "    score = float( elem[2] ) \n",
        "    inp_example = InputExample(texts=[elem[0], elem[1]], label=score)\n",
        "    train_samples.append(inp_example)\n",
        "\n",
        "\n",
        "  ## Params\n",
        "  train_batch_size = 4\n",
        "    \n",
        "  model            = SentenceTransformer( model_path )\n",
        "  train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
        "  train_loss       = losses.CosineSimilarityLoss(model=model)\n",
        "  \n",
        " # Train the model\n",
        "  dev_sims = eval_sims = results = None\n",
        "  if epoch is None :\n",
        "    ## Going to test all epochs - notice we can't use the default evaluator. \n",
        "    for epoch in range( 1, 10 ) :\n",
        "      warmup_steps     = math.ceil(len(train_dataloader) * epoch  * 0.1) #10% of train data for warm-up\n",
        "      print(\"Warmup-steps: {}\".format(warmup_steps), flush=True)\n",
        "      \n",
        "      model_save_path = os.path.join( out_location, str( seed ), str( epoch ) ) \n",
        "      model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "                evaluator=None,\n",
        "                epochs=1,\n",
        "                evaluation_steps=0,\n",
        "                warmup_steps=warmup_steps,\n",
        "                output_path=model_save_path\n",
        "      )\n",
        "\n",
        "      dev_sims  = get_similarities( dev_location , model, languages ) \n",
        "      eval_sims = get_similarities( eval_location, model, languages )\n",
        "\n",
        "      ## Create submission file on the development set. \n",
        "      submission_data = insert_to_submission( languages, [ 'fine_tune' ], dev_sims, dev_formated_file_location )  \n",
        "      results_file    = os.path.join( outpath, 'dev.combined_results-' + str( seed ) + '.csv' )\n",
        "      write_csv( submission_data, results_file )\n",
        "\n",
        "      ## Evaluate development set. \n",
        "      results = evaluate_submission( results_file, os.path.join( data_location, 'dev.gold.csv' ) )\n",
        "\n",
        "      ## Make results printable. \n",
        "      for result in results : \n",
        "        for result_index in range( 2, 5 ) : \n",
        "          result[result_index] = 'Did Not Attempt' if result[result_index] is None else result[ result_index ]\n",
        "  \n",
        "      for row in results : \n",
        "        print( '\\t'.join( [str(i) for i in row ] ) )\n",
        "        \n",
        "      results_file = os.path.join( model_save_path, 'RESULTS_TABLE-dev.pre_train_' + str(epoch) + str( seed ) + '.csv' )    \n",
        "      write_csv( results, results_file )      \n",
        "\n",
        "      ## Generate combined output for this epoch.\n",
        "      submission_data = insert_to_submission( languages, [ 'fine_tune' ], eval_sims, eval_formated_file_location )  \n",
        "      results_file    = os.path.join( outpath, 'eval.combined_results-' + str( seed ) + '_' + str( epoch ) + '.csv' )\n",
        "      write_csv( submission_data, results_file )\n",
        "\n",
        " \n",
        "  else :\n",
        "    ## We already know the best epoch and so will use it.\n",
        "    warmup_steps     = math.ceil(len(train_dataloader) * epoch  * 0.1) #10% of train data for warm-up\n",
        "    print(\"Warmup-steps: {}\".format(warmup_steps), flush=True)\n",
        "\n",
        "    model_save_path = os.path.join( out_location, str( seed ), str( epoch ) ) \n",
        "    model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "              evaluator=None,\n",
        "              epochs=epoch,\n",
        "              evaluation_steps=0,\n",
        "              warmup_steps=warmup_steps,\n",
        "              output_path=model_save_path\n",
        "    )\n",
        "    \n",
        "    dev_sims  = get_similarities( dev_location , model, languages ) \n",
        "    eval_sims = get_similarities( eval_location, model, languages )\n",
        "\n",
        "    ## Create submission file on the development set. \n",
        "    submission_data = insert_to_submission( languages, [ 'fine_tune' ], dev_sims, dev_formated_file_location )  \n",
        "    results_file    = os.path.join( outpath, 'dev.combined_results-' + str( seed ) + '.csv' )\n",
        "    write_csv( submission_data, results_file )\n",
        "    \n",
        "    ## Evaluate development set. \n",
        "    results = evaluate_submission( results_file, os.path.join( data_location, 'dev.gold.csv' ) )\n",
        "    \n",
        "    ## Make results printable. \n",
        "    # for result in results : \n",
        "    #   for result_index in range( 2, 5 ) : \n",
        "    #     result[result_index] = 'Did Not Attempt' if result[result_index] is None else result[ result_index ]\n",
        "  \n",
        "    # results_file = os.path.join( model_save_path, 'RESULTS_TABLE-dev.pre_train_' + str(epoch) + str( seed ) + '.csv' )    \n",
        "    # write_csv( results, results_file )\n",
        "\n",
        "\n",
        "\n",
        "  ## Outside if\n",
        "  return results"
      ],
      "metadata": {
        "id": "thKAyaZWYpmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'model_path'                  : model_path, \n",
        "    'seed'                        : 4, \n",
        "    'data_location'               : data_location, \n",
        "    'dev_formated_file_location'  : '/content/models/dev.pre_train_results-4.csv',  ## We can append to this.\n",
        "    'eval_formated_file_location' : '/content/models/eval.pre_train_results-4.csv',\n",
        "    'train_data'                  : train_data , \n",
        "    'out_location'                : out_location ,  \n",
        "    'languages'                   : languages ,\n",
        "    'epoch'                       : 4 ,\n",
        "} "
      ],
      "metadata": {
        "id": "g4TFQxe1Yw87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results=create_and_eval_subtask_b_fine_tune(**params)"
      ],
      "metadata": {
        "id": "W9Z-kKSNYxpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "hFdAIGJfY22X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}